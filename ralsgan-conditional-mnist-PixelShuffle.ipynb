{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 100\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./previews'):\n",
    "    os.makedirs('./previews')\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.makedirs('./checkpoints')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_class = 10\n",
    "n_class_remap = 28\n",
    "batch_size = 32\n",
    "n_dim = 32\n",
    "n_feature = 100\n",
    "n_ch = 1\n",
    "g_feature_map_b = 64\n",
    "d_feature_map_b = 64\n",
    "'''\n",
    "fold_dataset = datasets.ImageFolder('/hdd/dataset/pixiv_face_tagged', \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.RandomAffine(5, translate=(0.05,0.05), scale=(0.9,1.1), shear=2, resample=2, fillcolor=tuple([127]*n_ch)),\n",
    "                           transforms.Resize([n_dim]*2, interpolation=2),\n",
    "                           transforms.ToTensor(), # normalize to [0,1]\n",
    "                           transforms.Normalize([0.5]*n_ch, [0.5]*n_ch) # [0,1] -> [-1,+1]\n",
    "                       ]))\n",
    "'''\n",
    "fold_dataset = datasets.MNIST('./mnist_data', download=True, train=False, transform=transforms.Compose([\n",
    "                           transforms.Pad(2), # 28 -> 32\n",
    "                           transforms.ToTensor(), # normalize to [0,1]\n",
    "                           transforms.Normalize([0.5]*n_ch, [0.5]*n_ch) # [0,1] -> [-1,+1]\n",
    "                       ]))\n",
    "\n",
    "print(fold_dataset.__getitem__(100)[0].shape)\n",
    "plt.imshow(np.squeeze(np.clip(np.array(fold_dataset.__getitem__(100)[0]).transpose(1,2,0)*127.5+127.5,0,255).astype(np.uint8)))\n",
    "plt.show()\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        fold_dataset,\n",
    "        batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "print(n_dim, n_feature)\n",
    "def inf_data_gen():\n",
    "    while True:\n",
    "        for data, label in data_loader:\n",
    "            yield data, label\n",
    "gen = inf_data_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "\n",
    "def one_hot(ids, n_class):\n",
    "    if len(ids.size())==2:\n",
    "        return ids\n",
    "    ohe = torch.FloatTensor(ids.size(0), n_class, device=\"cpu\")\n",
    "    ids = ids.view(-1,1)\n",
    "    ohe.zero_()\n",
    "    ohe.scatter_(1, ids, 1)\n",
    "    return ohe\n",
    "\n",
    "def gradient_penalty_loss(C, real, fake, label, lambda_, device):\n",
    "    w = torch.rand(real.size(0), 1, 1, 1, device=device).expand_as(real)\n",
    "    x_hat = w*real + (1-w)*fake\n",
    "    x_hat_g = torch.autograd.Variable(x_hat, requires_grad=True)\n",
    "    D_x_hat_g = C(x_hat_g, label)\n",
    "    gradients = torch.autograd.grad(outputs=D_x_hat_g, inputs=x_hat_g,\n",
    "                              grad_outputs=torch.ones_like(D_x_hat_g, device=device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "    return gradient_penalty\n",
    "\n",
    "def soft_consistency_loss(C, r1, l1, lambda_, M, device):\n",
    "    r2 = torch.cat((r1[-1:], r1[:-1]), 0)\n",
    "    l2 = torch.cat((l1[-1:], l1[:-1]), 0)\n",
    "    C_r1 , C_r2 = C(r1,l1).view(r1.size(0), 1) , C(r2,l2).view(r2.size(0), 1)\n",
    "    r1_f , r2_f = r1.view(r1.size(0), -1) , r2.view(r2.size(0), -1)\n",
    "    return ( (C_r1-C_r2).norm(2, dim=1) / (r1_f-r2_f).norm(2, dim=1) - M ).clamp(min=0) # hinge???????\n",
    "\n",
    "class C(nn.Module):\n",
    "    def __init__(self, N_DIM, N_CH, BASE_FEATURE_N=32, N_CLASS=1, N_EMBEDDING=1):\n",
    "        super(C, self).__init__()\n",
    "        self.n_dim = N_DIM\n",
    "        self.n_ch  = N_CH\n",
    "        self.n_class = N_CLASS\n",
    "        self.n_emb = N_EMBEDDING\n",
    "        self.base_f = BASE_FEATURE_N\n",
    "        self.aux   = nn.Linear(self.n_class, self.n_emb, bias=False)\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "        self.conv_head = nn.Conv2d(self.n_ch+self.n_emb, self.base_f, 3, stride=1, padding=1, bias=False) \n",
    "        self.conv1 = nn.Conv2d(self.base_f, self.base_f, 3, stride=2, padding=1, bias=False) \n",
    "        self.conv2 = nn.Conv2d(self.base_f, self.base_f*2, 3, stride=2, padding=1, bias=False) \n",
    "        self.in2   = nn.InstanceNorm2d(self.base_f*2)\n",
    "        self.conv2_1 = nn.Conv2d(self.base_f*2, self.base_f*2, 3, stride=1, padding=1, bias=False) \n",
    "        self.in2_1   = nn.InstanceNorm2d(self.base_f*2)\n",
    "        self.conv3 = nn.Conv2d(self.base_f*2+self.n_emb, self.base_f*4, 3, stride=2, padding=1, bias=False) \n",
    "        self.in3   = nn.InstanceNorm2d(self.base_f*4)\n",
    "        self.conv4 = nn.Conv2d(self.base_f*4, self.base_f*4, 3, stride=2, padding=1, bias=False) \n",
    "        self.in4   = nn.InstanceNorm2d(self.base_f*4)\n",
    "        self.conv5 = nn.Conv2d(self.base_f*4, self.base_f*8, 3, stride=1, padding=1, bias=False) \n",
    "        self.in5   = nn.InstanceNorm2d(self.base_f*8)\n",
    "        self.fc1   = nn.Conv2d(self.base_f*8, 1, self.n_dim//16, stride=1, padding=0, bias=False)\n",
    "    def forward(self, x, label):\n",
    "        \n",
    "        label_emb   = self.aux(label)\n",
    "        label_ohe2d = label_emb.view(label_emb.size(0), label_emb.size(1), 1, 1)\n",
    "        \n",
    "        x = torch.cat((x, label_ohe2d.expand(label_emb.size(0),label_emb.size(1),x.size(2),x.size(3))), 1)\n",
    "        x = self.conv_head(x) \n",
    "        x = self.lrelu(x)\n",
    "        x = self.conv1(x) \n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.conv2(x) \n",
    "        x = self.in2(x) \n",
    "        x = self.lrelu(x)\n",
    "        x = self.conv2_1(x) \n",
    "        x = self.in2_1(x) \n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = torch.cat((x, label_ohe2d.expand(label_emb.size(0),label_emb.size(1),x.size(2),x.size(3))), 1)\n",
    "        x = self.conv3(x) \n",
    "        x = self.in3(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.conv4(x) \n",
    "        x = self.in4(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.conv5(x) \n",
    "        x = self.in5(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        x = self.fc1(x) \n",
    "        x = x.view(x.size(0), 1) \n",
    "        return x\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self, N_DIM, N_FEATURE, N_CH, BASE_FEATURE_N=32, N_CLASS=1, N_EMB=1):\n",
    "        super(G, self).__init__()\n",
    "        self.n_dim = N_DIM\n",
    "        self.n_ch  = N_CH\n",
    "        self.n_class = N_CLASS\n",
    "        self.base_f = BASE_FEATURE_N\n",
    "        self.n_feature = N_FEATURE\n",
    "        self.n_emb = N_EMB\n",
    "        self.aux   = nn.Linear(self.n_class, self.n_emb, bias=False)\n",
    "        self.conv_head = nn.Conv2d((self.n_emb+self.n_feature)//((self.n_dim//32)**2), self.base_f*8, 3, stride=1, padding=1, bias=False) \n",
    "        self.up1   = nn.PixelShuffle(2)\n",
    "        self.conv1 = nn.Conv2d(self.base_f*2, self.base_f*4, 3, stride=1, padding=1, bias=False) \n",
    "        self.in1   = nn.InstanceNorm2d(self.base_f*4)\n",
    "        self.conv1_1 = nn.Conv2d(self.base_f*4, self.base_f*4, 3, stride=1, padding=1, bias=False) \n",
    "        self.in1_1   = nn.InstanceNorm2d(self.base_f*4)\n",
    "        self.up2   = nn.PixelShuffle(2)\n",
    "        self.conv2 = nn.Conv2d(self.base_f, self.base_f*2, 3, stride=1, padding=1, bias=False) \n",
    "        self.in2   = nn.InstanceNorm2d(self.base_f*2)\n",
    "        self.conv2_1 = nn.Conv2d(self.base_f*2, self.base_f*2, 3, stride=1, padding=1, bias=False) \n",
    "        self.in2_1   = nn.InstanceNorm2d(self.base_f*2)\n",
    "        self.up3   = nn.PixelShuffle(2)\n",
    "        self.conv3 = nn.Conv2d(self.base_f//2, self.base_f, 3, stride=1, padding=1, bias=False) \n",
    "        self.in3   = nn.InstanceNorm2d(self.base_f)\n",
    "        self.up4   = nn.PixelShuffle(2)\n",
    "        self.conv4 = nn.Conv2d(self.base_f//4, self.base_f, 3, stride=1, padding=1, bias=False) \n",
    "        self.in4   = nn.InstanceNorm2d(self.base_f)\n",
    "        self.up5   = nn.PixelShuffle(2)\n",
    "        self.conv5 = nn.Conv2d(self.base_f//4, self.n_ch, 3, stride=1, padding=1, bias=False) \n",
    "    def forward(self, x, label):\n",
    "        label_emb = self.aux(label)\n",
    "        x = torch.cat((x, label_emb), 1)\n",
    "        x = x.view(x.size(0), x.size(1)//((self.n_dim//32)**2), (self.n_dim//32), (self.n_dim//32)) \n",
    "        \n",
    "        x = self.conv_head(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.conv1(x) \n",
    "        x = self.in1(x) \n",
    "        x = F.relu(x)\n",
    "        s = x\n",
    "        x = self.conv1_1(x) \n",
    "        x = x + s\n",
    "        x = self.in1_1(x) \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = self.conv2(x)  \n",
    "        x = self.in2(x)\n",
    "        x = F.relu(x)\n",
    "        s = x\n",
    "        x = self.conv2_1(x)  \n",
    "        x = x + s\n",
    "        x = self.in2_1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = self.conv3(x)  \n",
    "        x = self.in3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        x = self.conv4(x) \n",
    "        x = self.in4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.up5(x)\n",
    "        x = self.conv5(x)  \n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "def D(C, r, f, l):\n",
    "    return C(r,l) - C(f,l).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2dir(directory='./previews', imgs=None, iter_n=0):\n",
    "    imgs = np.clip(np.round((np.concatenate(tuple(imgs.transpose(0,2,3,1)), axis=0)+1)*127.5), 0, 255).astype(np.uint8) # (?, 28, 28)\n",
    "    cv2.imwrite('{}/{:08d}.jpg'.format(directory, iter_n), np.squeeze(imgs[...,::-1])) # RGB->BGR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3 # debug!!!\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "G_net = G(n_dim, n_feature, n_ch, g_feature_map_b, n_class, n_class_remap).to(device).apply(weights_init)\n",
    "C_net = C(n_dim, n_ch, d_feature_map_b, n_class, n_class_remap).to(device).apply(weights_init)\n",
    "opt_C = optim.Adam(C_net.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "opt_G = optim.Adam(G_net.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbe5931f8d846fdaa21bb91a041bb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200/6000] G: 7.7443, D:0.0629 -- elapsed_G: 0.0423s -- elapsed_D: 0.0392s\n",
      "[400/6000] G: 7.3935, D:0.0630 -- elapsed_G: 0.0425s -- elapsed_D: 0.0390s\n",
      "[600/6000] G: 7.3337, D:0.0954 -- elapsed_G: 0.0426s -- elapsed_D: 0.0394s\n",
      "[800/6000] G: 8.5627, D:0.1043 -- elapsed_G: 0.0427s -- elapsed_D: 0.0391s\n",
      "[1000/6000] G: 8.1096, D:0.1027 -- elapsed_G: 0.0426s -- elapsed_D: 0.0393s\n",
      "[1200/6000] G: 8.1480, D:0.1077 -- elapsed_G: 0.0428s -- elapsed_D: 0.0395s\n",
      "[1400/6000] G: 7.6963, D:0.1003 -- elapsed_G: 0.0425s -- elapsed_D: 0.0392s\n",
      "[1600/6000] G: 7.9846, D:0.1460 -- elapsed_G: 0.0427s -- elapsed_D: 0.0395s\n",
      "[1800/6000] G: 8.0008, D:0.1850 -- elapsed_G: 0.0463s -- elapsed_D: 0.0427s\n",
      "[2000/6000] G: 7.7969, D:0.2972 -- elapsed_G: 0.0428s -- elapsed_D: 0.0400s\n",
      "[2200/6000] G: 8.5954, D:0.2168 -- elapsed_G: 0.0429s -- elapsed_D: 0.0393s\n",
      "[2400/6000] G: 7.7635, D:0.2668 -- elapsed_G: 0.0430s -- elapsed_D: 0.0406s\n",
      "[2600/6000] G: 7.8276, D:0.3537 -- elapsed_G: 0.0427s -- elapsed_D: 0.0389s\n",
      "[2800/6000] G: 8.0189, D:0.4912 -- elapsed_G: 0.0427s -- elapsed_D: 0.0407s\n",
      "[3000/6000] G: 6.4116, D:0.4924 -- elapsed_G: 0.0428s -- elapsed_D: 0.0407s\n",
      "[3200/6000] G: 7.3033, D:0.5509 -- elapsed_G: 0.0437s -- elapsed_D: 0.0402s\n",
      "[3400/6000] G: 6.3647, D:0.3651 -- elapsed_G: 0.0429s -- elapsed_D: 0.0407s\n",
      "[3600/6000] G: 6.8493, D:0.6020 -- elapsed_G: 0.1652s -- elapsed_D: 0.0283s\n",
      "[3800/6000] G: 7.6443, D:0.2846 -- elapsed_G: 0.0435s -- elapsed_D: 0.0392s\n",
      "[4000/6000] G: 8.7459, D:0.3367 -- elapsed_G: 0.0428s -- elapsed_D: 0.0393s\n",
      "[4200/6000] G: 7.2532, D:0.3682 -- elapsed_G: 0.0430s -- elapsed_D: 0.0401s\n",
      "[4400/6000] G: 5.6229, D:0.2919 -- elapsed_G: 0.0427s -- elapsed_D: 0.0397s\n",
      "[4600/6000] G: 6.7765, D:0.3769 -- elapsed_G: 0.0427s -- elapsed_D: 0.0447s\n",
      "[4800/6000] G: 7.1362, D:0.3532 -- elapsed_G: 0.0428s -- elapsed_D: 0.0395s\n",
      "[5000/6000] G: 8.1727, D:0.5570 -- elapsed_G: 0.0441s -- elapsed_D: 0.0416s\n",
      "[5200/6000] G: 8.4847, D:0.3531 -- elapsed_G: 0.0425s -- elapsed_D: 0.0394s\n",
      "[5400/6000] G: 7.4587, D:0.2248 -- elapsed_G: 0.0428s -- elapsed_D: 0.0395s\n",
      "[5600/6000] G: 7.2086, D:0.4620 -- elapsed_G: 0.0428s -- elapsed_D: 0.0393s\n",
      "[5800/6000] G: 6.6158, D:0.7981 -- elapsed_G: 0.0453s -- elapsed_D: 0.0411s\n",
      "[6000/6000] G: 7.8856, D:0.2667 -- elapsed_G: 0.0427s -- elapsed_D: 0.0394s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "iterations = 6000\n",
    "preview_iter = 200\n",
    "# max_preview_imgs = 5\n",
    "d_iter = 1\n",
    "std = 1.0\n",
    "lambda_1 , lambda_2 = 10 , 0.2\n",
    "M = 0.05\n",
    "\n",
    "for ite in tqdm_notebook(range(1, iterations+1)):\n",
    "    start_train_ts = time.time()\n",
    "    # train D:\n",
    "    G_net.eval()\n",
    "    C_net.train()\n",
    "    d_loss_mean = 0.0\n",
    "    g_loss_mean = 0.0\n",
    "    for _ in range(d_iter):\n",
    "        opt_C.zero_grad()\n",
    "        real, label = next(gen)\n",
    "        real = real.to(device)\n",
    "        label = one_hot(label, n_class).to(device)\n",
    "        sample = torch.randn(real.size(0), n_feature, device=device).clamp(-2,2) * std\n",
    "        fake   = G_net(sample, label).detach() # not to touch G_net\n",
    "        d_loss_real = ((D(C_net,real,fake,label) - 1)**2).mean()\n",
    "        d_loss_real.backward()\n",
    "        d_loss_fake = ((D(C_net,fake,real,label) + 1)**2).mean()\n",
    "        d_loss_fake.backward()\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        opt_C.step()\n",
    "        d_loss_mean += d_loss.item()\n",
    "    d_loss_mean /= d_iter\n",
    "    D_update_ts = time.time()\n",
    "    # train G:\n",
    "    real, label = next(gen)\n",
    "    real = real.to(device)\n",
    "    label = one_hot(label, n_class).to(device)\n",
    "    G_net.train()\n",
    "    C_net.eval()\n",
    "    opt_G.zero_grad()\n",
    "    sample = torch.randn(real.size(0), n_feature, device=device).clamp(-2,2) * std\n",
    "    generated = G_net(sample, label)\n",
    "    g_loss = ((D(C_net,generated,real,label) - 1)**2).mean() + ((D(C_net,real,generated,label) + 1)**2).mean()\n",
    "    g_loss.backward()\n",
    "    opt_G.step()\n",
    "    g_loss_mean = g_loss.mean().item()\n",
    "    G_update_ts = time.time()\n",
    "    if ite%preview_iter==0:\n",
    "        print('[{}/{}] G: {:.4f}, D:{:.4f} -- elapsed_G: {:.4f}s -- elapsed_D: {:.4f}s'.format(ite, iterations, g_loss_mean, d_loss_mean, (G_update_ts-D_update_ts), (D_update_ts-start_train_ts) ))\n",
    "        \n",
    "        imgs = []\n",
    "        for c in range(n_class):\n",
    "            sample = torch.randn(1, n_feature, device=device).clamp(-2,2) * std\n",
    "            label  = one_hot(torch.LongTensor([c], device=\"cpu\"), n_class).to(device)\n",
    "            generated = G_net(sample, label).detach().cpu().numpy()[0]\n",
    "            imgs.append(generated)\n",
    "        imgs = np.asarray(imgs)\n",
    "        \n",
    "        plot2dir('./previews', imgs, ite)\n",
    "        torch.save(G_net.state_dict(), './checkpoints/iter-{:d}-G.ckpt'.format(ite))\n",
    "        torch.save(C_net.state_dict(), './checkpoints/iter-{:d}-D.ckpt'.format(ite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
