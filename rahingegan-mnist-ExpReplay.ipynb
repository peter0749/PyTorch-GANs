{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 100\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./previews'):\n",
    "    os.makedirs('./previews')\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.makedirs('./checkpoints')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "n_dim = 32\n",
    "n_feature = 100\n",
    "n_ch = 1\n",
    "g_feature_map_b = 64\n",
    "d_feature_map_b = 64\n",
    "'''\n",
    "fold_dataset = datasets.ImageFolder('./pixiv_face_tagged', \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.RandomAffine(5, translate=(0.05,0.05), scale=(0.9,1.1), shear=2, resample=2, fillcolor=tuple([127]*n_ch)),\n",
    "                           transforms.Resize([n_dim]*2, interpolation=2),\n",
    "                           transforms.ToTensor(), # normalize to [0,1]\n",
    "                           transforms.Normalize([0.5]*n_ch, [0.5]*n_ch) # [0,1] -> [-1,+1]\n",
    "                       ]))\n",
    "'''\n",
    "fold_dataset = datasets.MNIST('./mnist_data', download=True, train=False, transform=transforms.Compose([\n",
    "                           transforms.Pad(2), # 28 -> 32\n",
    "                           transforms.ToTensor(), # normalize to [0,1]\n",
    "                           transforms.Normalize([0.5]*n_ch, [0.5]*n_ch) # [0,1] -> [-1,+1]\n",
    "                       ]))\n",
    "\n",
    "print(fold_dataset.__getitem__(100)[0].shape)\n",
    "plt.imshow(np.squeeze(np.clip(np.array(fold_dataset.__getitem__(100)[0]).transpose(1,2,0)*127.5+127.5,0,255).astype(np.uint8)))\n",
    "plt.show()\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        fold_dataset,\n",
    "        batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "print(n_dim, n_feature)\n",
    "def inf_data_gen():\n",
    "    while True:\n",
    "        for data, label in data_loader:\n",
    "            yield data\n",
    "gen = inf_data_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "\n",
    "def one_hot(ids, n_class):\n",
    "    if len(ids.size())==2:\n",
    "        return ids\n",
    "    ohe = torch.FloatTensor(ids.size(0), n_class)\n",
    "    ids = ids.view(-1,1)\n",
    "    ohe.zero_()\n",
    "    ohe.scatter_(1, ids, 1)\n",
    "    return ohe\n",
    "\n",
    "class ConvolutioBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, norm=True, down=False, relu=True, leaky=False, dropout=False):\n",
    "        super(ConvolutioBlock, self).__init__()\n",
    "        \n",
    "        conv_block = []\n",
    "        conv_block += [nn.Conv2d(in_ch, out_ch, 3, stride=2 if down else 1, padding=1, bias=False)]\n",
    "        if norm:\n",
    "            conv_block += [nn.InstanceNorm2d(out_ch)]\n",
    "        if relu:\n",
    "            conv_block += [ nn.LeakyReLU(0.2, inplace=True) if leaky else nn.ReLU(inplace=True) ]\n",
    "        if dropout:\n",
    "            conv_block += [nn.Dropout(0.1)]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "        self.conv_block.apply(weights_init)\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "# ref SRResNet\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [  nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1, bias=False),\n",
    "                        nn.InstanceNorm2d(in_ch),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1, bias=False),\n",
    "                        nn.InstanceNorm2d(in_ch)  \n",
    "                     ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "        self.conv_block.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class UpConvolution(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, norm=True, relu=True):\n",
    "        super(UpConvolution, self).__init__()\n",
    "        \n",
    "        conv_block = [nn.PixelShuffle(2)]\n",
    "        conv_block += [nn.Conv2d(in_ch//4, out_ch, 3, stride=1, padding=1, bias=False)]\n",
    "        if norm:\n",
    "            conv_block += [nn.InstanceNorm2d(out_ch)]\n",
    "        if relu:\n",
    "            conv_block += [nn.ReLU(inplace=True)]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "        self.conv_block.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "class C(nn.Module):\n",
    "    def __init__(self, N_DIM=32, N_CH=3, BASE_FEATURE_N=32):\n",
    "        super(C, self).__init__()\n",
    "        self.n_dim = N_DIM\n",
    "        self.n_ch = N_CH\n",
    "        self.base_f = BASE_FEATURE_N\n",
    "        self.head_conv = nn.Conv2d(self.n_ch, self.base_f, 3, stride=1, padding=1, bias=False)\n",
    "        weights_init(self.head_conv)\n",
    "        self.convs = nn.Sequential(*[\n",
    "            ConvolutioBlock( self.base_f, self.base_f,   norm=True, down=True, relu=True, leaky=True, dropout=True ),\n",
    "            ConvolutioBlock( self.base_f, self.base_f*2, norm=True, down=True, relu=True, leaky=True, dropout=True ),\n",
    "            ConvolutioBlock( self.base_f*2, self.base_f*4, norm=True, down=True, relu=True, leaky=True, dropout=True ),\n",
    "            ConvolutioBlock( self.base_f*4, self.base_f*8, norm=True, down=True, relu=True, leaky=True, dropout=True ),\n",
    "        ])\n",
    "        self.tail_conv = nn.Conv2d(self.base_f*8, 1, self.n_dim//16, stride=1, padding=0, bias=False)\n",
    "        weights_init(self.tail_conv)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.head_conv(x)\n",
    "        x = self.convs(x)\n",
    "        x = self.tail_conv (x)\n",
    "        x = x.view(x.size(0), 1) \n",
    "        return x\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self, N_DIM, N_FEATURE, N_CH, BASE_FEATURE_N=32):\n",
    "        super(G, self).__init__()\n",
    "        self.n_dim = N_DIM\n",
    "        self.n_ch  = N_CH\n",
    "        self.base_f = BASE_FEATURE_N\n",
    "        self.n_feature = N_FEATURE\n",
    "        self.latent_map = nn.Linear(self.n_feature, self.base_f*8*((self.n_dim//32)**2)) \n",
    "        weights_init(self.latent_map)\n",
    "        self.convs = nn.Sequential(*[\n",
    "            ResidualBlock(self.base_f*8),\n",
    "            UpConvolution(self.base_f*8, self.base_f*4, norm=True, relu=True), # 2x\n",
    "            UpConvolution(self.base_f*4, self.base_f*4, norm=True, relu=True), # 4x\n",
    "            UpConvolution(self.base_f*4, self.base_f*2, norm=True, relu=True), # 8x\n",
    "            UpConvolution(self.base_f*2, self.base_f*2, norm=True, relu=True), # 16x\n",
    "            UpConvolution(self.base_f*2, self.base_f, norm=True, relu=True),   # 32x\n",
    "        ])\n",
    "        self.tail_conv = nn.Conv2d(self.base_f, self.n_ch, 3, stride=1, padding=1, bias=False)\n",
    "        weights_init(self.tail_conv)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.latent_map(x)\n",
    "        x = x.view(x.size(0), self.base_f*8, self.n_dim//32, self.n_dim//32)\n",
    "        x = self.convs(x)\n",
    "        x = self.tail_conv (x)\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "def D(C, r, f):\n",
    "    return C(r) - C(f).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2dir(directory='./previews', imgs=None, iter_n=0):\n",
    "    imgs = np.clip(np.round((np.concatenate(tuple(imgs.transpose(0,2,3,1)), axis=0)+1)*127.5), 0, 255).astype(np.uint8) # (?, 28, 28)\n",
    "    cv2.imwrite('{}/{:08d}.jpg'.format(directory, iter_n), np.squeeze(imgs[...,::-1])) # RGB->BGR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3 # debug!!!\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "G_net = G(n_dim, n_feature, n_ch, g_feature_map_b).to(device)\n",
    "C_net = C(n_dim, n_ch, d_feature_map_b).to(device)\n",
    "opt_C = optim.Adam(C_net.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "opt_G = optim.Adam(G_net.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b09a0cd1104545807a91930efb2cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200/5000] G: 9.4003, D:0.0694 -- elapsed_G: 0.0527s -- elapsed_D: 0.0618s\n",
      "[400/5000] G: 9.0287, D:0.0000 -- elapsed_G: 0.0528s -- elapsed_D: 0.0340s\n",
      "[600/5000] G: 11.1221, D:0.1045 -- elapsed_G: 0.0548s -- elapsed_D: 0.0355s\n",
      "[800/5000] G: 10.7239, D:0.1665 -- elapsed_G: 0.0527s -- elapsed_D: 0.0351s\n",
      "[1000/5000] G: 8.4010, D:0.3059 -- elapsed_G: 0.0521s -- elapsed_D: 0.0334s\n",
      "[1200/5000] G: 9.3189, D:0.2847 -- elapsed_G: 0.0526s -- elapsed_D: 0.0336s\n",
      "[1400/5000] G: 10.7835, D:0.0972 -- elapsed_G: 0.0522s -- elapsed_D: 0.0610s\n",
      "[1600/5000] G: 10.7127, D:0.0586 -- elapsed_G: 0.0528s -- elapsed_D: 0.0345s\n",
      "[1800/5000] G: 9.5162, D:0.2232 -- elapsed_G: 0.0526s -- elapsed_D: 0.0613s\n",
      "[2000/5000] G: 6.5744, D:0.2318 -- elapsed_G: 0.0530s -- elapsed_D: 0.0358s\n",
      "[2200/5000] G: 9.4176, D:0.1151 -- elapsed_G: 0.0526s -- elapsed_D: 0.0340s\n",
      "[2400/5000] G: 7.2633, D:0.1347 -- elapsed_G: 0.0532s -- elapsed_D: 0.0338s\n",
      "[2600/5000] G: 9.2281, D:0.2627 -- elapsed_G: 0.0526s -- elapsed_D: 0.0339s\n",
      "[2800/5000] G: 8.1240, D:0.1399 -- elapsed_G: 0.0524s -- elapsed_D: 0.0339s\n",
      "[3000/5000] G: 9.8125, D:0.3567 -- elapsed_G: 0.0527s -- elapsed_D: 0.0360s\n",
      "[3200/5000] G: 7.6104, D:0.6461 -- elapsed_G: 0.0527s -- elapsed_D: 0.0354s\n",
      "[3400/5000] G: 6.1532, D:0.6176 -- elapsed_G: 0.0530s -- elapsed_D: 0.0352s\n",
      "[3600/5000] G: 6.4356, D:0.3635 -- elapsed_G: 0.2113s -- elapsed_D: 0.0687s\n",
      "[3800/5000] G: 6.8184, D:0.2201 -- elapsed_G: 0.0522s -- elapsed_D: 0.0364s\n",
      "[4000/5000] G: 7.1023, D:0.1201 -- elapsed_G: 0.0527s -- elapsed_D: 0.0616s\n",
      "[4200/5000] G: 7.6951, D:0.3468 -- elapsed_G: 0.0524s -- elapsed_D: 0.0636s\n",
      "[4400/5000] G: 6.0531, D:0.1727 -- elapsed_G: 0.0525s -- elapsed_D: 0.0339s\n",
      "[4600/5000] G: 8.0343, D:0.2898 -- elapsed_G: 0.0525s -- elapsed_D: 0.0336s\n",
      "[4800/5000] G: 6.6995, D:0.6267 -- elapsed_G: 0.0524s -- elapsed_D: 0.0339s\n",
      "[5000/5000] G: 6.9769, D:0.3555 -- elapsed_G: 0.0531s -- elapsed_D: 0.0355s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "iterations = 5000\n",
    "preview_iter = 200\n",
    "preview_imgs = 8\n",
    "d_iter = 1\n",
    "std = 1.0\n",
    "lambda_1 , lambda_2 = 10 , 0.2\n",
    "M = 0.05\n",
    "exp_replay_maxl = 100\n",
    "exp_replay = deque(maxlen=exp_replay_maxl)\n",
    "\n",
    "for ite in tqdm_notebook(range(1, iterations+1)):\n",
    "    start_train_ts = time.time()\n",
    "    # train D:\n",
    "    G_net.eval()\n",
    "    C_net.train()\n",
    "    d_loss_mean = []\n",
    "    g_loss_mean = 0.0\n",
    "    for _ in range(d_iter):\n",
    "        opt_C.zero_grad()\n",
    "        real = next(gen).to(device)\n",
    "        sample = torch.randn(real.size(0), n_feature, device=device).clamp(-2,2) * std\n",
    "        fake   = G_net(sample).detach() # not to touch G_net\n",
    "        d_loss_real = ( 1-D(C_net, real,fake) ).clamp(min=0).mean()\n",
    "        d_loss_real.backward()\n",
    "        d_loss_fake = ( 1+D(C_net, fake,real) ).clamp(min=0).mean()\n",
    "        d_loss_fake.backward()\n",
    "        d_loss = d_loss_real + d_loss_fake # + d_loss_gp\n",
    "        opt_C.step()\n",
    "        d_loss_mean.append(d_loss.item())\n",
    "        exp_replay.append(( real.detach().cpu(), sample.detach().cpu(), fake.detach().cpu() ))\n",
    "    if len(exp_replay)>=exp_replay_maxl and np.random.rand()<0.2:\n",
    "        opt_C.zero_grad()\n",
    "        idx = np.random.randint(len(exp_replay)-1)\n",
    "        real_, sample_, fake_ = exp_replay[idx]\n",
    "        real_ = real_.to(device)\n",
    "        sample_ = sample_.to(device)\n",
    "        fake_ = fake_.to(device)\n",
    "        d_loss_real = ( 1-D(C_net, real_,fake_) ).clamp(min=0).mean()\n",
    "        d_loss_real.backward()\n",
    "        d_loss_fake = ( 1+D(C_net, fake_,real_) ).clamp(min=0).mean()\n",
    "        d_loss_fake.backward()\n",
    "        d_loss = d_loss_real + d_loss_fake # + d_loss_gp\n",
    "        opt_C.step()\n",
    "        d_loss_mean.append(d_loss.item())\n",
    "    d_loss_mean = np.mean(d_loss_mean)\n",
    "    D_update_ts = time.time()\n",
    "    # train G:\n",
    "    real = next(gen).to(device)\n",
    "    G_net.train()\n",
    "    C_net.train() # activate Discriminator's Dropout \n",
    "    opt_G.zero_grad()\n",
    "    sample = torch.randn(real.size(0), n_feature, device=device).clamp(-2,2) * std\n",
    "    generated = G_net(sample)\n",
    "    g_loss = ( 1-D(C_net, generated,real) ).clamp(min=0).mean() + ( 1+D(C_net, real,generated) ).clamp(min=0).mean()\n",
    "    g_loss.backward()\n",
    "    opt_G.step()\n",
    "    g_loss_mean = g_loss.mean().item()\n",
    "    G_update_ts = time.time()\n",
    "    if ite%preview_iter==0:\n",
    "        print('[{}/{}] G: {:.4f}, D:{:.4f} -- elapsed_G: {:.4f}s -- elapsed_D: {:.4f}s'.format(ite, iterations, g_loss_mean, d_loss_mean, (G_update_ts-D_update_ts), (D_update_ts-start_train_ts) ))\n",
    "        \n",
    "        G_net.eval() # evaluation state\n",
    "        sample = torch.randn(preview_imgs, n_feature, device=device).clamp(-2,2) * std\n",
    "        generated = G_net(sample).detach().cpu().numpy()\n",
    "        plot2dir('./previews', generated, ite)\n",
    "        torch.save(G_net.state_dict(), './checkpoints/iter-{:d}-G.ckpt'.format(ite))\n",
    "        torch.save(C_net.state_dict(), './checkpoints/iter-{:d}-D.ckpt'.format(ite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
